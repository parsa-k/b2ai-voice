{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-10T19:05:36.280799Z",
     "iopub.status.busy": "2024-07-10T19:05:36.280467Z",
     "iopub.status.idle": "2024-07-10T19:05:36.285668Z",
     "shell.execute_reply": "2024-07-10T19:05:36.285005Z",
     "shell.execute_reply.started": "2024-07-10T19:05:36.280773Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import uuid\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchaudio\n",
    "from torchvision.models import resnet18\n",
    "from b2aiprep.process import Audio, specgram, plot_spectrogram, plot_waveform\n",
    "import IPython.display as Ipd\n",
    "#from torchsummary import summary\n",
    "\n",
    "import numpy as np\n",
    "from pydub import AudioSegment\n",
    "import librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-10T19:05:36.423769Z",
     "iopub.status.busy": "2024-07-10T19:05:36.423519Z",
     "iopub.status.idle": "2024-07-10T19:05:36.443820Z",
     "shell.execute_reply": "2024-07-10T19:05:36.443110Z",
     "shell.execute_reply.started": "2024-07-10T19:05:36.423745Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def determine_is_stridor_and_phonatory(name, folder_path, patient_status_df):\n",
    "    \"\"\"\n",
    "    Determine if the path should be marked as stridor and phonatory based on the folder structure and excel sheet.\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    \n",
    "    for part in folder_path.split(os.sep):\n",
    "        if part.upper() in [\"CONTROL\", \"CONTROLS\"]:\n",
    "            return 0, 0, 0  # is_stridor, is_phonatory, status\n",
    "\n",
    "    # Extract patient code from the name\n",
    "    if name.startswith(\"Patient \"):\n",
    "        patient_code = int(name.split(\" \")[1])\n",
    "        if patient_code in patient_status_df['Code'].values:\n",
    "            status = patient_status_df[patient_status_df['Code'] == patient_code]['Status'].values[0]\n",
    "            if status == \"No-Stridor\":\n",
    "                return 0, 0, 0  # is_stridor, is_phonatory, status\n",
    "            elif status == \"Phonatory\":\n",
    "                return 1, 1, 2  # is_stridor, is_phonatory, status\n",
    "            elif status == \"Non-Phonatory\":\n",
    "                return 1, 0, 1  # is_stridor, is_phonatory, status\n",
    "            \n",
    "            \n",
    "    #global print_once\n",
    "    #if print_once:\n",
    "        #print(\" WARNING: Patents 1,5, 10 do not found but gavien 0 , 0 , 0 status\")\n",
    "        #print_once = False\n",
    "    #print('patient not found', name, folder_path)\n",
    "    \n",
    "    return 0, 0, 0  # Default to non-stridor if not found\n",
    "\n",
    "def get_audio_duration(file_path):\n",
    "    \"\"\"\n",
    "    Get the duration of the audio file in seconds.\n",
    "    \"\"\"\n",
    "    audio = AudioSegment.from_wav(file_path)\n",
    "    return len(audio) / 1000  # Convert milliseconds to seconds\n",
    "\n",
    "def load_audio(file_path):\n",
    "    \"\"\"\n",
    "    Load the audio file.\n",
    "    \"\"\"\n",
    "    y, sr = librosa.load(file_path)\n",
    "    return y, sr\n",
    "\n",
    "def compute_short_time_energy(y, frame_size=1024, hop_length=512):\n",
    "    \"\"\"\n",
    "    Compute the short-time energy of the audio signal.\n",
    "    \"\"\"\n",
    "    energy = np.array([\n",
    "        np.sum(np.square(y[i:i+frame_size]))\n",
    "        for i in range(0, len(y), hop_length)\n",
    "    ])\n",
    "    return energy\n",
    "\n",
    "def find_most_active_timeframe(y, sr, hop_length, total_duration, slot_duration):\n",
    "    \"\"\"\n",
    "    Find the most active timeframe in the audio signal and break it into slots.\n",
    "    \"\"\"\n",
    "    frame_duration = int(slot_duration * sr / hop_length)\n",
    "    total_frames = int(total_duration * sr / hop_length)\n",
    "    energy = compute_short_time_energy(y, frame_size=hop_length, hop_length=hop_length)\n",
    "    \n",
    "    # Find the most active timeframe\n",
    "    active_start_frame = np.argmax(np.convolve(energy, np.ones(total_frames), mode='valid'))\n",
    "    active_start_time = (active_start_frame * hop_length) / sr\n",
    "    active_end_time = active_start_time + total_duration\n",
    "\n",
    "    # Break the active timeframe into slots\n",
    "    slots = []\n",
    "    for i in range(total_frames // frame_duration):\n",
    "        slot_start_time = active_start_time + i * slot_duration\n",
    "        slot_end_time = slot_start_time + slot_duration\n",
    "        slots.append({\"start_time\": slot_start_time, \"end_time\": slot_end_time})\n",
    "    \n",
    "    return slots\n",
    "\n",
    "def generate_recording_objects(root_path, patient_status_df):\n",
    "    recordings = []\n",
    "\n",
    "    # Traverse the directory structure\n",
    "    for root, dirs, files in os.walk(root_path):\n",
    "        # Check if there are wav files in the current directory\n",
    "        wav_files = [file for file in files if file.endswith('.wav')]\n",
    "        if wav_files:\n",
    "            # Get the parent folder name\n",
    "            parent_folder = os.path.basename(root)\n",
    "            \n",
    "            for wav_file in wav_files:\n",
    "                if parent_folder in [\"Patient 1\", \"Patient 5\", \"Patient 10\"]:\n",
    "                    #print(f\"parent_folder: {parent_folder}\")\n",
    "                    continue\n",
    "                # Generate a unique ID for each player-session-recording combination\n",
    "                uid = str(uuid.uuid4())\n",
    "                # Extract the recording label (base name without extension)\n",
    "                recording_label = os.path.splitext(wav_file)[0]\n",
    "                # Get the full path of the wav file\n",
    "                file_path = os.path.join(root, wav_file)\n",
    "                # Get the duration of the audio file\n",
    "                duration = get_audio_duration(file_path)\n",
    "\n",
    "                # Calculate the number of slots and their duration\n",
    "                num_slots = duration // 3\n",
    "                if num_slots == 0:\n",
    "                    continue  # Skip recordings that are too short\n",
    "                total_duration = num_slots * 3\n",
    "                \n",
    "                # Load the audio file and compute its short-time energy\n",
    "                y, sr = load_audio(file_path)\n",
    "                slots = find_most_active_timeframe(y, sr, hop_length=512, total_duration=total_duration, slot_duration=3)\n",
    "                \n",
    "                # Determine is_stridor and is_phonatory from the folder structure and Excel sheet\n",
    "                is_stridor, is_phonatory, status = determine_is_stridor_and_phonatory(parent_folder, root, patient_status_df)\n",
    "                \n",
    "                # Create the entry for the current recording\n",
    "                entry = {\n",
    "                    \"uid\": uid,\n",
    "                    \"name\": parent_folder,\n",
    "                    \"recording\": file_path,  # Include the full path\n",
    "                    \"recording_label\": recording_label,\n",
    "                    \"is_stridor\": is_stridor,\n",
    "                    \"is_phonatory\": is_phonatory,\n",
    "                    \"status\": status,\n",
    "                    \"duration\": duration,\n",
    "                    \"recording_slots\": slots\n",
    "                }\n",
    "                recordings.append(entry)\n",
    "                \n",
    "    return recordings\n",
    "\n",
    "def save_to_csv(recordings, output_csv):\n",
    "    # Flatten the recording slots for CSV\n",
    "    flattened_data = []\n",
    "    for record in recordings:\n",
    "        base_data = {key: record[key] for key in record if key != 'recording_slots'}\n",
    "        for slot in record['recording_slots']:\n",
    "            slot_data = base_data.copy()\n",
    "            slot_data.update(slot)\n",
    "            flattened_data.append(slot_data)\n",
    "    \n",
    "    df = pd.DataFrame(flattened_data)\n",
    "    df.to_csv(output_csv, index=False)\n",
    "\n",
    "def save_to_json(recordings, output_json):\n",
    "    with open(output_json, 'w') as json_file:\n",
    "        json.dump(recordings, json_file, indent=4)\n",
    "\n",
    "def count_samples(recordings):\n",
    "    no_stridor_count = 0\n",
    "    phonatory_count = 0\n",
    "    non_phonatory_count = 0\n",
    "\n",
    "    for record in recordings:\n",
    "        if record['status'] == 0:\n",
    "            no_stridor_count += 1\n",
    "        elif record['status'] == 2:\n",
    "            phonatory_count += 1\n",
    "        elif record['status'] == 1:\n",
    "            non_phonatory_count += 1\n",
    "    \n",
    "    return no_stridor_count, phonatory_count, non_phonatory_count\n",
    "\n",
    "\n",
    "def filter_recordings_by_label(data, label_prefix):\n",
    "    \"\"\"\n",
    "    Filter recordings to only include those with recording_label starting with the specified prefix.\n",
    "    Exclude specific labels when the prefix is 'FIMO'.\n",
    "    \"\"\"\n",
    "    exclude_labels = {\"FIMO\": [\"FIMOcricoid\", \"FIMOthyroid\", \"FIMOC\", \"FIMOT\"]}\n",
    "\n",
    "    filtered_data = []\n",
    "    for recording in data:\n",
    "        if recording['recording_label'].startswith(label_prefix):\n",
    "            if label_prefix == \"FIMO\" and recording['recording_label'] in exclude_labels[\"FIMO\"]:\n",
    "                continue\n",
    "            filtered_data.append(recording)\n",
    "\n",
    "    return filtered_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-10T19:05:36.602051Z",
     "iopub.status.busy": "2024-07-10T19:05:36.601782Z",
     "iopub.status.idle": "2024-07-10T19:05:50.770966Z",
     "shell.execute_reply": "2024-07-10T19:05:50.770323Z",
     "shell.execute_reply.started": "2024-07-10T19:05:36.602028Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No-Stridor samples: 350\n",
      "Phonatory samples: 89\n",
      "Non-Phonatory samples: 267\n"
     ]
    }
   ],
   "source": [
    "# Usage\n",
    "root_path = '/data/parsak/stridor/Data files'  # Replace with the actual path to your folders\n",
    "excel_path = '/data/parsak/stridor/Stridor_Labels (1).xlsx'  # Replace with the actual path to your Excel file\n",
    "output_csv = 'rec_5_time.csv'  # Desired output CSV file name\n",
    "output_json = 'rec_5_time.json'  # Desired output JSON file name\n",
    "\n",
    "# Read the Excel file\n",
    "patient_status_df = pd.read_excel(excel_path)\n",
    "\n",
    "#print_once = True\n",
    "recordings = generate_recording_objects(root_path, patient_status_df)\n",
    "\n",
    "# Uncomment the following lines to save to CSV\n",
    "# save_to_csv(recordings, output_csv)\n",
    "# print(f\"Recording objects have been saved to {output_csv}\")\n",
    "\n",
    "# Uncomment the following lines to save to JSON\n",
    "# save_to_json(recordings, output_json)\n",
    "# print(f\"Recording objects have been saved to {output_json}\")\n",
    "\n",
    "# Count the samples\n",
    "no_stridor_count, phonatory_count, non_phonatory_count = count_samples(recordings)\n",
    "print(f\"No-Stridor samples: {no_stridor_count}\")\n",
    "print(f\"Phonatory samples: {phonatory_count}\")\n",
    "print(f\"Non-Phonatory samples: {non_phonatory_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-10T19:05:50.772358Z",
     "iopub.status.busy": "2024-07-10T19:05:50.772092Z",
     "iopub.status.idle": "2024-07-10T19:05:50.778314Z",
     "shell.execute_reply": "2024-07-10T19:05:50.777783Z",
     "shell.execute_reply.started": "2024-07-10T19:05:50.772337Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Filter the data to only include recordings with recording_label starting with \"FIMO\"\n",
    "filtered_data = filter_recordings_by_label(recordings, \"FIMO\")\n",
    "\n",
    "df = pd.DataFrame(recordings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-10T19:44:05.842471Z",
     "iopub.status.busy": "2024-07-10T19:44:05.842216Z",
     "iopub.status.idle": "2024-07-10T19:44:05.855765Z",
     "shell.execute_reply": "2024-07-10T19:44:05.855056Z",
     "shell.execute_reply.started": "2024-07-10T19:44:05.842448Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MySpecgramDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataframe, label, classification, segment_size=3):\n",
    "        self.dataframe = dataframe\n",
    "        self.label = label\n",
    "        self.classification = classification\n",
    "        self.segment_size = segment_size\n",
    "        \n",
    "        # Ensure the DataFrame has the necessary columns\n",
    "        required_columns = ['uid', 'recording', 'recording_label', self.label]\n",
    "        for col in required_columns:\n",
    "            if col not in dataframe.columns:\n",
    "                raise ValueError(f\"DataFrame is missing required column: {col}\")\n",
    "\n",
    "        # Extract necessary information from the DataFrame\n",
    "        self.audio_files = dataframe['recording'].tolist()\n",
    "        self.labels = dataframe[self.label].tolist()\n",
    "        self.uids = dataframe['uid'].tolist()\n",
    "        self.recording_labels = dataframe['recording_label'].tolist()\n",
    "        \n",
    "        if classification == 'stridor**nonstridor':\n",
    "            self.num_classes = 2\n",
    "        elif classification == 'nonstridor**phonatory**nonphonatory':\n",
    "            self.num_classes = 3\n",
    "        else:\n",
    "            raise ValueError(\"Error: type 'stridor**nonstridor' Or 'nonstridor**phonatory**nonphonatory' for classification\")\n",
    "            \n",
    "        if (label == 'is_stridor' and classification == 'stridor**nonstridor') or (label == 'status' and classification == 'nonstridor**phonatory**nonphonatory'):\n",
    "            pass\n",
    "        else:\n",
    "            raise ValueError(\"label and classification are NOT match\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        uid = self.uids[idx]\n",
    "        audio_file = self.audio_files[idx]\n",
    "        label = self.labels[idx]\n",
    "        recording_label = self.recording_labels[idx]\n",
    "        num_classes = self.num_classes\n",
    "\n",
    "        try:\n",
    "            audio = Audio.from_file(self.audio_files[idx])\n",
    "            audio = audio.to_16khz()\n",
    "            \n",
    "           \n",
    "            # get middle K seconds if audio is too long, pad with zeros if it is too short\n",
    "            if audio.signal.size(0) > self.segment_size * 16000:\n",
    "                d = (audio.signal.size(0) - self.segment_size * 16000) // 2\n",
    "                audio.signal = audio.signal[d:d + self.segment_size * 16000]\n",
    "            else:\n",
    "                #print(\"before\", audio.signal.size())\n",
    "                padding_needed = self.segment_size*16000-audio.signal.size(0)\n",
    "                audio.signal = F.pad(audio.signal, (0, 0, 0, padding_needed)) \n",
    "                #audio.signal = torch.nn.functional.pad(audio.signal, (0, self.segment_size*16000-audio.signal.size(0)), mode='constant', value=0)\n",
    "                #print(\"after\",audio.signal.size())\n",
    "            \n",
    "            \n",
    "            win_length = 60\n",
    "            #print(audio.signal.size(1))\n",
    "            hop_length = 1\n",
    "            nfft = int(win_length*16)\n",
    "            features_specgram = specgram(audio, win_length=win_length, hop_length=hop_length, n_fft=nfft)\n",
    "            features_specgram = 10.0 * torch.log10(torch.maximum(features_specgram, torch.full(features_specgram.size(), fill_value=1e-10))).T\n",
    "            \n",
    "            features_specgram = torch.nn.functional.interpolate(features_specgram.unsqueeze(0).unsqueeze(0), size=(512, 512), mode='bilinear', align_corners=False).squeeze(0).squeeze(0)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {audio_file}: {e}\")\n",
    "            return self.__getitem__((idx + 1) % len(self))\n",
    "\n",
    "        # Convert the label to a tensor\n",
    "\n",
    "        \n",
    "        label = torch.tensor(label, dtype=torch.long)\n",
    "        label = F.one_hot(label, num_classes=num_classes).float()\n",
    "\n",
    "        return {'uid': uid, 'signal': features_specgram, 'audio': audio.signal, self.label: label, 'recording_label': recording_label}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-10T19:44:17.887675Z",
     "iopub.status.busy": "2024-07-10T19:44:17.887423Z",
     "iopub.status.idle": "2024-07-10T19:44:18.740112Z",
     "shell.execute_reply": "2024-07-10T19:44:18.739488Z",
     "shell.execute_reply.started": "2024-07-10T19:44:17.887650Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset: <__main__.MySpecgramDataset object at 0x14b7a5924550>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dataset = MySpecgramDataset(df, label = 'is_stridor',  classification = 'stridor**nonstridor', segment_size = 5) # classification:  'stridor**nonstridor' Or 'nonstridor**phonatory**nonphonatory' \n",
    "label = 'is_stridor' # 'is_stridor' or 'status'\n",
    "number_classes = 2\n",
    "#classes = ['no-stridor', 'non-phonatory', 'phonatory']\n",
    "classes = ['no-stridor', 'stridor']\n",
    "\n",
    "\n",
    "print(f\"dataset: {dataset}\")\n",
    "\n",
    "# Split the dataset into train, validation, and test sets\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = int(0.1 * len(dataset))\n",
    "test_size = len(dataset) - train_size - val_size\n",
    "train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, val_size, test_size])\n",
    "\n",
    "# Create DataLoaders for batching and shuffling\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "# Example: Iterate through the train DataLoader\n",
    "for batch in train_dataloader:\n",
    "    \n",
    "    #print(batch['recording_label'], batch[label])\n",
    "    #print('Label is: ', batch[label][0])\n",
    "    #for i in range(1,5):\n",
    "    #    plot_spectrogram(batch['signal'][i])\n",
    "    #    plot_waveform(batch['audio'][i], 16000, title=\"Waveform\", ax=None)\n",
    "    #print(\"the shape is: \", batch['signal'][0].shape)\n",
    "    #print(\"the size is: \", batch['signal'].shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-10T19:44:19.858498Z",
     "iopub.status.busy": "2024-07-10T19:44:19.858238Z",
     "iopub.status.idle": "2024-07-10T19:44:19.862709Z",
     "shell.execute_reply": "2024-07-10T19:44:19.862020Z",
     "shell.execute_reply.started": "2024-07-10T19:44:19.858473Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-10T19:44:20.271436Z",
     "iopub.status.busy": "2024-07-10T19:44:20.271186Z",
     "iopub.status.idle": "2024-07-10T19:44:20.290162Z",
     "shell.execute_reply": "2024-07-10T19:44:20.289516Z",
     "shell.execute_reply.started": "2024-07-10T19:44:20.271412Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "from einops import rearrange, repeat\n",
    "from einops.layers.torch import Rearrange\n",
    "\n",
    "# helpers\n",
    "\n",
    "def pair(t):\n",
    "    return t if isinstance(t, tuple) else (t, t)\n",
    "\n",
    "# classes\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, dim, hidden_dim, dropout = 0.):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.LayerNorm(dim),\n",
    "            nn.Linear(dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, heads = 8, dim_head = 64, dropout = 0.):\n",
    "        super().__init__()\n",
    "        inner_dim = dim_head *  heads\n",
    "        project_out = not (heads == 1 and dim_head == dim)\n",
    "\n",
    "        self.heads = heads\n",
    "        self.scale = dim_head ** -0.5\n",
    "\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "\n",
    "        self.attend = nn.Softmax(dim = -1)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias = False)\n",
    "\n",
    "        self.to_out = nn.Sequential(\n",
    "            nn.Linear(inner_dim, dim),\n",
    "            nn.Dropout(dropout)\n",
    "        ) if project_out else nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.norm(x)\n",
    "\n",
    "        qkv = self.to_qkv(x).chunk(3, dim = -1)\n",
    "        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h = self.heads), qkv)\n",
    "\n",
    "        dots = torch.matmul(q, k.transpose(-1, -2)) * self.scale\n",
    "\n",
    "        attn = self.attend(dots)\n",
    "        attn = self.dropout(attn)\n",
    "\n",
    "        out = torch.matmul(attn, v)\n",
    "        out = rearrange(out, 'b h n d -> b n (h d)')\n",
    "        return self.to_out(out)\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, dim, depth, heads, dim_head, mlp_dim, dropout = 0.):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        self.layers = nn.ModuleList([])\n",
    "        for _ in range(depth):\n",
    "            self.layers.append(nn.ModuleList([\n",
    "                Attention(dim, heads = heads, dim_head = dim_head, dropout = dropout),\n",
    "                FeedForward(dim, mlp_dim, dropout = dropout)\n",
    "            ]))\n",
    "\n",
    "    def forward(self, x):\n",
    "        for attn, ff in self.layers:\n",
    "            x = attn(x) + x\n",
    "            x = ff(x) + x\n",
    "\n",
    "        return self.norm(x)\n",
    "\n",
    "class ViT(nn.Module):\n",
    "    def __init__(self, *, image_size, patch_size, num_classes, dim, depth, heads, mlp_dim, pool = 'cls', channels = 1, dim_head = 64, dropout = 0., emb_dropout = 0.):\n",
    "        super().__init__()\n",
    "        image_height, image_width = pair(image_size)\n",
    "        patch_height, patch_width = pair(patch_size)\n",
    "\n",
    "        assert image_height % patch_height == 0 and image_width % patch_width == 0, 'Image dimensions must be divisible by the patch size.'\n",
    "\n",
    "        num_patches = (image_height // patch_height) * (image_width // patch_width)\n",
    "        patch_dim = channels * patch_height * patch_width\n",
    "        assert pool in {'cls', 'mean'}, 'pool type must be either cls (cls token) or mean (mean pooling)'\n",
    "\n",
    "        self.to_patch_embedding = nn.Sequential(\n",
    "            Rearrange('b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1 = patch_height, p2 = patch_width),\n",
    "            nn.LayerNorm(patch_dim),\n",
    "            nn.Linear(patch_dim, dim),\n",
    "            nn.LayerNorm(dim),\n",
    "        )\n",
    "\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, num_patches + 1, dim))\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, dim))\n",
    "        self.dropout = nn.Dropout(emb_dropout)\n",
    "\n",
    "        self.transformer = Transformer(dim, depth, heads, dim_head, mlp_dim, dropout)\n",
    "\n",
    "        self.pool = pool\n",
    "        self.to_latent = nn.Identity()\n",
    "\n",
    "        self.mlp_head = nn.Linear(dim, num_classes)\n",
    "\n",
    "    def forward(self, img):\n",
    "        x = self.to_patch_embedding(img)\n",
    "        b, n, _ = x.shape\n",
    "\n",
    "        cls_tokens = repeat(self.cls_token, '1 1 d -> b 1 d', b = b)\n",
    "        x = torch.cat((cls_tokens, x), dim=1)\n",
    "        x += self.pos_embedding[:, :(n + 1)]\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.transformer(x)\n",
    "\n",
    "        x = x.mean(dim = 1) if self.pool == 'mean' else x[:, 0]\n",
    "\n",
    "        x = self.to_latent(x)\n",
    "        return self.mlp_head(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-10T19:44:20.802526Z",
     "iopub.status.busy": "2024-07-10T19:44:20.802275Z",
     "iopub.status.idle": "2024-07-10T19:44:21.300777Z",
     "shell.execute_reply": "2024-07-10T19:44:21.300005Z",
     "shell.execute_reply.started": "2024-07-10T19:44:20.802503Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = ViT(\n",
    "    image_size = 512,\n",
    "    patch_size = 64,\n",
    "    num_classes = number_classes,\n",
    "    dim = 1024,\n",
    "    depth = 6,\n",
    "    heads = 16,\n",
    "    mlp_dim = 2048,\n",
    "    dropout = 0.1,\n",
    "    emb_dropout = 0.1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-10T19:44:21.823922Z",
     "iopub.status.busy": "2024-07-10T19:44:21.823663Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 TrainLoss: 0.7987 TrainACC: 0.6436 ValACC: 0.6571\n",
      "Saved!\n",
      "Epoch: 2 TrainLoss: 0.6683 TrainACC: 0.5975 ValACC: 0.6143\n",
      "Epoch: 3 TrainLoss: 0.6969 TrainACC: 0.7234 ValACC: 0.6429\n",
      "Epoch: 4 TrainLoss: 0.4541 TrainACC: 0.8670 ValACC: 0.6286\n",
      "Epoch: 5 TrainLoss: 0.2505 TrainACC: 0.9450 ValACC: 0.6714\n",
      "Saved!\n",
      "Epoch: 6 TrainLoss: 0.1925 TrainACC: 0.9965 ValACC: 0.6714\n",
      "Saved!\n",
      "Epoch: 7 TrainLoss: 0.1243 TrainACC: 0.9025 ValACC: 0.6143\n",
      "Epoch: 8 TrainLoss: 0.2200 TrainACC: 0.9255 ValACC: 0.6143\n",
      "Epoch: 9 TrainLoss: 0.0661 TrainACC: 0.9982 ValACC: 0.7286\n",
      "Saved!\n",
      "Epoch: 10 TrainLoss: 0.0048 TrainACC: 1.0000 ValACC: 0.6714\n",
      "Epoch: 11 TrainLoss: 0.0013 TrainACC: 1.0000 ValACC: 0.6571\n",
      "Epoch: 12 TrainLoss: 0.0007 TrainACC: 1.0000 ValACC: 0.6571\n",
      "Epoch: 13 TrainLoss: 0.0006 TrainACC: 1.0000 ValACC: 0.6571\n",
      "Epoch: 14 TrainLoss: 0.0004 TrainACC: 1.0000 ValACC: 0.6571\n",
      "Epoch: 15 TrainLoss: 0.0003 TrainACC: 1.0000 ValACC: 0.6571\n",
      "Epoch: 16 TrainLoss: 0.0002 TrainACC: 1.0000 ValACC: 0.6571\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def eval(model, dataloader, device):\n",
    "    model.eval()\n",
    "    acc = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            signal = batch['signal'].to(device)\n",
    "            labels = batch[label].to(device)\n",
    "\n",
    "            outputs = model(signal.view(-1, 1, signal.size(1), signal.size(2)))\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            _, true_labels = torch.max(labels, 1)\n",
    "            \n",
    "            acc += (predicted == true_labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "    return acc / total\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)  # Move model to GPU if available\n",
    "\n",
    "num_epochs = 20\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001, weight_decay=5e-5)\n",
    "\n",
    "best_val_acc = 0\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    closs = []\n",
    "    for batch in train_dataloader:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        signal = batch['signal'].to(device)  # Move batch to GPU\n",
    "        labels = batch[label].to(device)  # Move labels to GPU\n",
    "        outputs = model(signal.view(-1, 1, signal.size(1), signal.size(2)))\n",
    "\n",
    "        # Convert labels to long before calculating cross entropy loss\n",
    "        _, true_labels = torch.max(labels, 1)\n",
    "        loss = torch.nn.functional.cross_entropy(outputs, true_labels)\n",
    "        \n",
    "        closs.append(loss.item())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    train_acc = eval(model, train_dataloader, device)\n",
    "    val_acc = eval(model, val_dataloader, device)\n",
    "    print(f'Epoch: {epoch+1} TrainLoss: {sum(closs)/len(closs):.4f} TrainACC: {train_acc:.4f} ValACC: {val_acc:.4f}')\n",
    "\n",
    "    if val_acc >= best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        torch.save(model.state_dict(), './model.pth')\n",
    "        print('Saved!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1\n",
    "2\n",
    "3\n",
    "4\n",
    "5\n",
    "6\n",
    "7\n",
    "8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-07-10T19:42:20.662628Z",
     "iopub.status.idle": "2024-07-10T19:42:20.662908Z",
     "shell.execute_reply": "2024-07-10T19:42:20.662776Z",
     "shell.execute_reply.started": "2024-07-10T19:42:20.662762Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import torch\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "def get_predictions_and_labels(model, dataloader, device):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            signal = batch['signal'].to(device)\n",
    "            outputs = model(signal.view(-1, 1, signal.size(1), signal.size(2)).to(device))\n",
    "            preds = torch.argmax(outputs, dim=1).cpu().numpy()\n",
    "            labels = torch.argmax(batch[label], dim=1).cpu().numpy()  # Convert one-hot encoded labels to class indices\n",
    "            all_preds.extend(preds)\n",
    "            all_labels.extend(labels)\n",
    "    return all_preds, all_labels\n",
    "\n",
    "# Assuming test_dataloader is defined and model is trained\n",
    "model.load_state_dict(torch.load('./ViT_2_5s.pth'))\n",
    "\n",
    "# Get predictions and true labels for the test set\n",
    "preds, labels = get_predictions_and_labels(model, test_dataloader, device)\n",
    "\n",
    "# Compute confusion matrix and classification report\n",
    "conf_matrix = confusion_matrix(labels, preds)\n",
    "class_report = classification_report(labels, preds, target_names=classes)\n",
    "\n",
    "# Plot the confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=classes, yticklabels=classes)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "print('Classification Report:\\n', class_report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-07-10T19:13:21.820204Z",
     "iopub.status.idle": "2024-07-10T19:13:21.820502Z",
     "shell.execute_reply": "2024-07-10T19:13:21.820357Z",
     "shell.execute_reply.started": "2024-07-10T19:13:21.820344Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "preds, labels = get_predictions_and_labels(model, val_dataloader, device)\n",
    "print("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-07-10T15:59:21.243669Z",
     "iopub.status.idle": "2024-07-10T15:59:21.243993Z",
     "shell.execute_reply": "2024-07-10T15:59:21.243850Z",
     "shell.execute_reply.started": "2024-07-10T15:59:21.243837Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "F.one_hot(torch.tensor([0,1,2,0,1,1,2], dtype=torch.long), num_classes=3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-b2ai]",
   "language": "python",
   "name": "conda-env-.conda-b2ai-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
